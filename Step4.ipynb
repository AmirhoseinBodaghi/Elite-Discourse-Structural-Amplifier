{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263e2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# MAIN DRIVER CONFIGURATION\n",
    "# ==============================================================\n",
    "\n",
    "BASE_INPUT_DIR = Path(r\" \")\n",
    "BASE_OUTPUT_DIR = Path(r\" \")\n",
    "INTERVENTION_DATE = datetime(2020, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d73d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# IMPORT YOUR EXISTING ANALYSIS FUNCTION\n",
    "# ==============================================================\n",
    "\n",
    "# def analyze_elite(FILE_PATH_INPUT, FILE_PATH_OUTPUT, INTERVENTION_DATE):\n",
    "def analyze_elite(FILE_PATH_INPUT, ELITE_OUT_DIR, INTERVENTION_DATE):\n",
    "    \"\"\"\n",
    "    This function runs your full analysis pipeline for one elite.\n",
    "    The code inside is exactly what you already had ‚Äî unchanged.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import spacy\n",
    "    import regex as re\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    from fuzzywuzzy import fuzz, process\n",
    "    import random\n",
    "    from pathlib import Path\n",
    "    from tqdm.auto import tqdm\n",
    "    from collections import Counter\n",
    "    from datetime import datetime\n",
    "    from community import community_louvain\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # ======= BEGIN your existing block =======\n",
    "\n",
    "    \"\"\"\n",
    "    elite_name = Path(FILE_PATH_INPUT).stem\n",
    "    ELITE_OUT_DIR = Path(FILE_PATH_OUTPUT) / elite_name\n",
    "    ELITE_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    ELITE_OUT_DIR = Path(ELITE_OUT_DIR)  # ‚Üê Ensure Path\n",
    "    ELITE_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    elite_name = Path(FILE_PATH_INPUT).stem\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    print(f\"\\nüöÄ Starting analysis for: {elite_name}\")\n",
    "    print(f\"üìÅ Outputs will be saved to: {ELITE_OUT_DIR}\")\n",
    "\n",
    "    COVID_KEYWORDS = {\n",
    "        \"en\": {\"covid\", \"covid19\", \"covid-19\", \"coronavirus\", \"pandemic\", \"sarscov2\", \"vaccine\", \"vaccination\"},\n",
    "        \"ar\": {\"ŸÉŸàÿ±ŸàŸÜÿß\", \"ŸÉŸàŸÅŸäÿØ\", \"ŸÉŸàŸÅŸäÿØ-19\", \"ÿ¨ÿßÿ¶ÿ≠ÿ© ŸÉŸàÿ±ŸàŸÜÿß\", \"Ÿàÿ®ÿßÿ° ŸÉŸàÿ±ŸàŸÜÿß\", \"ŸÅŸäÿ±Ÿàÿ≥ ŸÉŸàÿ±ŸàŸÜÿß\", \"ŸÑŸÇÿßÿ≠\", \"ÿ™ÿ∑ÿπŸäŸÖ\"},\n",
    "        \"es\": {\"covid\", \"covid-19\", \"coronavirus\", \"pandemia\", \"vacuna\", \"vacunaci√≥n\"},\n",
    "        \"pt\": {\"covid\", \"covid-19\", \"coronavirus\", \"pandemia\", \"vacina\", \"vacina√ß√£o\"},\n",
    "        \"fr\": {\"covid\", \"covid-19\", \"coronavirus\", \"pand√©mie\", \"vaccin\", \"vaccination\"},\n",
    "        \"he\": {\"◊ß◊ï◊®◊ï◊†◊î\", \"◊ß◊ï◊ë◊ô◊ì\", \"◊ß◊ï◊ë◊ô◊ì-19\", \"◊ï◊ô◊®◊ï◊°\", \"◊û◊í◊ô◊§◊î\", \"◊ó◊ô◊°◊ï◊ü\", \"◊ó◊ô◊°◊ï◊†◊ô◊ù\"},\n",
    "        \"tr\": {\"koronavir√ºs\", \"kovid\", \"kovid-19\", \"pandemi\", \"salgƒ±n\", \"a≈üƒ±\", \"a≈üƒ±lanma\"},\n",
    "        \"ko\": {\"ÏΩîÎ°úÎÇò\", \"ÏΩîÎ°úÎÇò19\", \"ÏΩîÎ°úÎÇòÎ∞îÏù¥Îü¨Ïä§\", \"Ìå¨Îç∞ÎØπ\", \"Î∞±Ïã†\", \"Ï†ëÏ¢Ö\", \"Ïò§ÎØ∏ÌÅ¨Î°†\"},\n",
    "        \"id\": {\"covid\", \"covid-19\", \"korona\", \"pandemi\", \"vaksin\", \"vaksinasi\"},\n",
    "        \"ur\": {\"⁄©Ÿàÿ±ŸàŸÜÿß\", \"⁄©ŸàŸà⁄à\", \"⁄©ŸàŸà⁄à-19\", \"Ÿàÿ®ÿßÿ°\", \"Ÿà€å⁄©ÿ≥€åŸÜ\", \"Ÿà€å⁄©ÿ≥€å ŸÜ€åÿ¥ŸÜ\", \"Ÿàÿßÿ¶ÿ±ÿ≥\"},\n",
    "        \"hi\": {\"‡§ï‡•ã‡§∞‡•ã‡§®‡§æ\", \"‡§ï‡•ã‡§µ‡§ø‡§°\", \"‡§ï‡•ã‡§µ‡§ø‡§°-19\", \"‡§Æ‡§π‡§æ‡§Æ‡§æ‡§∞‡•Ä\", \"‡§µ‡•à‡§ï‡•ç‡§∏‡•Ä‡§®\", \"‡§ü‡•Ä‡§ï‡§æ‡§ï‡§∞‡§£\"}\n",
    "    }\n",
    "\n",
    "    ALL_COVID_TERMS = set(term.lower() for terms in COVID_KEYWORDS.values() for term in terms)\n",
    "\n",
    "    # ========== LOAD DATA ==========\n",
    "    df = pd.read_excel(FILE_PATH_INPUT)\n",
    "    # Try to detect and parse the actual datetime column\n",
    "    if \"creation_datetime\" in df.columns:\n",
    "        # Replace the weird *** separator with a space\n",
    "        df[\"date\"] = (\n",
    "            df[\"creation_datetime\"]\n",
    "            .astype(str)\n",
    "            .str.replace(r\"\\*\\*\\*\", \" \", regex=True)\n",
    "        )\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    elif \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"date\"] = pd.NaT\n",
    "\n",
    "\n",
    "    # load spacy once per run\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "    except:\n",
    "        import spacy.cli\n",
    "        spacy.cli.download(\"en_core_web_lg\")\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "    # ========== CLEANING ==========\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)           # remove URLs\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)              # remove mentions\n",
    "        text = re.sub(r\"\\bRT\\b\", \"\", text)            # remove RT\n",
    "        text = re.sub(r\"[^\\p{L}\\p{N}\\s]\", \" \", text)  # keep only letters/numbers/spaces\n",
    "        text = re.sub(r\"\\b[\\p{N}]+\\b\", \" \", text)     # remove standalone numbers (Arabic or Western)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)              # normalize whitespace\n",
    "        return text.strip()\n",
    "\n",
    "    df[\"clean_text\"] = df[\"text\"].astype(str).apply(clean_text)\n",
    "\n",
    "    def extract_entities(text):\n",
    "        doc = nlp(text)\n",
    "        ents = [ent.text.strip() for ent in doc.ents if ent.label_ in [\n",
    "            \"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\",\n",
    "            \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\"\n",
    "        ]]\n",
    "        return list(set(ents))\n",
    "\n",
    "    df[\"entities\"] = df[\"clean_text\"].apply(extract_entities)\n",
    "\n",
    "    # ---------- EXTRACT ENTITIES WITH PROGRESS ----------\n",
    "    def extract_entities(text):\n",
    "        doc = nlp(text)\n",
    "        ents = [\n",
    "            ent.text.strip()\n",
    "            for ent in doc.ents\n",
    "            if ent.label_ in {\n",
    "                \"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\",\n",
    "                \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\"\n",
    "            }\n",
    "        ]\n",
    "        return list(set(ents))\n",
    "\n",
    "    # ---- NEW: apply with a manual loop + progress ----\n",
    "    PROGRESS_STEP = 100                     # change to 50, 500, ‚Ä¶ if you like\n",
    "    entities_col = []                       # will hold the list of entities per row\n",
    "\n",
    "    print(\"Extracting entities ‚Ä¶\")\n",
    "    for idx, txt in enumerate(df[\"clean_text\"].astype(str)):\n",
    "        entities_col.append(extract_entities(txt))\n",
    "\n",
    "        # print progress every PROGRESS_STEP rows\n",
    "        if (idx + 1) % PROGRESS_STEP == 0:\n",
    "            print(f\"   processed {idx + 1:,} tweets\")\n",
    "\n",
    "    df[\"entities\"] = entities_col\n",
    "    print(f\"   finished ‚Äì total {len(df):,} tweets\\n\")\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    # Flatten entities into dataframe (unchanged)\n",
    "    entities_list = [\n",
    "        (ent, idx) for idx, ents in enumerate(df[\"entities\"]) for ent in ents\n",
    "    ]\n",
    "    entities_df = pd.DataFrame(entities_list, columns=[\"entity\", \"tweet_index\"])\n",
    "\n",
    "    # --- Step: Rename 'covid' mentions that occur only before intervention ---\n",
    "    # Since we merge all COVID-related keywords after the intervention into a single entity named 'covid',\n",
    "    # any entity literally named 'covid' before the intervention would incorrectly merge with those post-intervention mentions.\n",
    "    # To prevent this overlap, we rename 'covid' mentions that occur before the intervention to 'covidbefore'.\n",
    "    # Other COVID-related keywords are not affected, as they keep their original names unless they appear after the intervention.\n",
    "\n",
    "    for ent in entities_df[\"entity\"].unique():\n",
    "        if ent.lower() == \"covid\":\n",
    "            tweet_idxs = entities_df.loc[entities_df[\"entity\"] == ent, \"tweet_index\"].unique()\n",
    "            tweet_dates = df[\"date\"].loc[tweet_idxs]\n",
    "            # For tweets before intervention\n",
    "            before_mask = tweet_dates < INTERVENTION_DATE\n",
    "            before_idxs = tweet_idxs[before_mask]\n",
    "            # Update those entity names to 'covidbefore'\n",
    "            entities_df.loc[entities_df[\"tweet_index\"].isin(before_idxs) &\n",
    "                            (entities_df[\"entity\"] == ent), \"entity\"] = \"covidbefore\"\n",
    "            \n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # SIMPLE FAST ENTITY RESOLUTION (Single-Core rapidfuzz + Progress)\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    from rapidfuzz import process, fuzz\n",
    "\n",
    "    def normalize_entities_fast(\n",
    "        entities_df,\n",
    "        covid_terms,\n",
    "        df_dates,\n",
    "        intervention_date,\n",
    "        threshold=90,\n",
    "        progress_step=100,\n",
    "    ):\n",
    "        print(\"Starting simple fast entity resolution (rapidfuzz)\")\n",
    "        uniq_entities = entities_df[\"entity\"].unique()\n",
    "        total = len(uniq_entities)\n",
    "        print(f\"   {total:,} unique entities to process\\n\")\n",
    "\n",
    "        # ---------- COVID merge (with progress) ----------\n",
    "        mapping = {}\n",
    "        non_covid = []\n",
    "\n",
    "        for i, ent in enumerate(uniq_entities):\n",
    "            ent_low = ent.lower()\n",
    "            tweet_idxs = entities_df.loc[entities_df[\"entity\"] == ent, \"tweet_index\"].unique()\n",
    "            tweet_dates = df_dates.loc[tweet_idxs]\n",
    "            occurs_after = (tweet_dates >= intervention_date).any()\n",
    "\n",
    "            if occurs_after and any(term in ent_low for term in covid_terms):\n",
    "                mapping[ent] = \"covid\"\n",
    "            else:\n",
    "                non_covid.append(ent)\n",
    "\n",
    "            if (i + 1) % progress_step == 0 or (i + 1) == total:\n",
    "                print(f\"   [COVID-merge] processed {i + 1:,}/{total:,} entities\")\n",
    "\n",
    "        n_covid = sum(1 for v in mapping.values() if v == \"covid\")\n",
    "        print(f\"   {n_covid:,} entities ‚Üí merged to 'covid'\\n\")\n",
    "\n",
    "        if not non_covid:\n",
    "            return mapping\n",
    "\n",
    "        # ---------- Simple fuzzy deduplication (single-core, fast) ----------\n",
    "        unique = []\n",
    "        print(f\"   Deduplicating {len(non_covid):,} non-COVID entities (rapidfuzz)‚Ä¶\")\n",
    "\n",
    "        for i, ent in enumerate(non_covid):\n",
    "            match = process.extractOne(ent, unique, scorer=fuzz.token_sort_ratio)\n",
    "            if match and match[1] >= threshold:\n",
    "                mapping[ent] = match[0]\n",
    "            else:\n",
    "                unique.append(ent)\n",
    "                mapping[ent] = ent\n",
    "\n",
    "            if (i + 1) % progress_step == 0 or (i + 1) == len(non_covid):\n",
    "                print(f\"   [Deduplication] processed {i + 1:,}/{len(non_covid):,} entities\")\n",
    "\n",
    "        print(\"\\nFast entity resolution complete!\\n\")\n",
    "        return mapping\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # CALL THE FUNCTION\n",
    "    # --------------------------------------------------------------\n",
    "    entity_mapping = normalize_entities_fast(\n",
    "        entities_df,\n",
    "        covid_terms=ALL_COVID_TERMS,\n",
    "        df_dates=df[\"date\"],\n",
    "        intervention_date=INTERVENTION_DATE,\n",
    "        threshold=90,\n",
    "        progress_step=100\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # APPLY THE MAPPING\n",
    "    # --------------------------------------------------------------\n",
    "    entities_df[\"entity_normalized\"] = entities_df[\"entity\"].map(entity_mapping)\n",
    "    df[\"entities_normalized\"] = df[\"entities\"].apply(\n",
    "        lambda lst: [entity_mapping.get(e, e) for e in lst]\n",
    "    )\n",
    "\n",
    "\n",
    "    # ========== CREATE NETWORK ==========\n",
    "    global G, covid_ego\n",
    "    G = nx.Graph()\n",
    "    \n",
    "\n",
    "    for idx, ents in enumerate(df[\"entities_normalized\"]):\n",
    "        ents_unique = list(set(ents))\n",
    "        for i in range(len(ents_unique)):\n",
    "            for j in range(i + 1, len(ents_unique)):\n",
    "                e1, e2 = ents_unique[i], ents_unique[j]\n",
    "                if G.has_edge(e1, e2):\n",
    "                    G[e1][e2][\"weight\"] += 1\n",
    "                else:\n",
    "                    G.add_edge(e1, e2, weight=1)\n",
    "\n",
    "    print(f\"‚úÖ Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "\n",
    "    # ========== COVID EGO EXTRACTION (Now Single Unified Node) ==========\n",
    "    if \"covid\" in G.nodes:\n",
    "        covid_ego = nx.ego_graph(G, \"covid\", radius=1)\n",
    "        print(f\"ü¶† COVID ego network has {covid_ego.number_of_nodes()} nodes and {covid_ego.number_of_edges()} edges.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No COVID-related entities found after normalization.\")\n",
    "        covid_ego = None\n",
    "\n",
    "    \n",
    "    # ==============================================================\n",
    "    # VISUALIZATION (Full Graph (comment out now) + COVID Ego Network)\n",
    "    # ==============================================================\n",
    "\n",
    "    def visualize_graph(G, title, filename, label_sample_size=25, out_dir=ELITE_OUT_DIR):\n",
    "        \"\"\"\n",
    "        Visualize a networkx graph in a dark, high-quality style.\n",
    "        Saves as a high-res PNG to the elite's output folder.\n",
    "        \"\"\"\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        import networkx as nx\n",
    "        import random\n",
    "        from pathlib import Path\n",
    "\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        plt.style.use(\"dark_background\")\n",
    "\n",
    "        # explicitly set dark background for both figure and axes\n",
    "        fig = plt.gcf()\n",
    "        ax = plt.gca()\n",
    "        fig.patch.set_facecolor('black')\n",
    "        ax.set_facecolor('black')\n",
    "\n",
    "        # Layout and styling\n",
    "        pos = nx.spring_layout(G, k=0.6, iterations=100, seed=42)\n",
    "        node_size = 30\n",
    "        edge_width = 4\n",
    "        edge_color = \"#FA8E8E\"\n",
    "        node_color = \"#FFFF00\"\n",
    "\n",
    "        # Label sampling\n",
    "        top_node = max(G.nodes(), key=lambda node: G.degree(node))\n",
    "        sampled_nodes = random.sample(list(G.nodes()), min(label_sample_size, len(G.nodes())))\n",
    "        if top_node not in sampled_nodes:\n",
    "            sampled_nodes.append(top_node)\n",
    "        labels = {node: node for node in sampled_nodes}\n",
    "        label_pos = {node: (x, y + 0.05) for node, (x, y) in pos.items() if node in labels}\n",
    "\n",
    "        # Draw\n",
    "        nx.draw_networkx_nodes(\n",
    "            G, pos, node_size=node_size, node_color=node_color,\n",
    "            edgecolors='white', linewidths=0.2, alpha=0.9\n",
    "        )\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos, width=edge_width, edge_color=edge_color, alpha=0.5\n",
    "        )\n",
    "        nx.draw_networkx_labels(\n",
    "            G, label_pos, labels=labels, font_size=18,\n",
    "            font_color='yellow', font_weight='bold'\n",
    "        )\n",
    "\n",
    "        plt.title(title, fontsize=18, color='white', pad=20)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save with transparent=False to preserve dark background\n",
    "        file_path = Path(out_dir) / filename\n",
    "        plt.savefig(file_path, dpi=400, bbox_inches='tight', facecolor=fig.get_facecolor(), transparent=False)\n",
    "        plt.close()\n",
    "        print(f\"‚úÖ Saved graph visualization: {file_path}\")\n",
    "        return file_path\n",
    "\n",
    "\n",
    "    # ---- Generate both visualizations ----\n",
    "    # if G is not None and G.number_of_nodes() > 0:\n",
    "        # visualize_graph(G, f\"Full Entity Graph ‚Äî {elite_name}\", f\"{elite_name}_FullGraph.png\")\n",
    "\n",
    "    if covid_ego is not None and covid_ego.number_of_nodes() > 0:\n",
    "        visualize_graph(covid_ego, f\"COVID Ego Network ‚Äî {elite_name}\", f\"{elite_name}_COVIDego.png\")\n",
    "\n",
    "    print(\"‚úÖ Visualizations complete.\")\n",
    "\n",
    "    \n",
    "    # ==============================================================\n",
    "    # FINAL ULTRA-CLEAN GRAPH METRICS (8 METRICS ONLY)\n",
    "    # ==============================================================\n",
    "\n",
    "    def graph_metrics(H: nx.Graph, sample_frac=0.3, seed=42):\n",
    "        n_nodes = H.number_of_nodes()\n",
    "        n_edges = H.number_of_edges()\n",
    "\n",
    "        if n_nodes == 0:\n",
    "            return {k: np.nan for k in [\n",
    "                \"n_nodes\", \"n_edges\", \"density\", \"clustering\", \"modularity\",\n",
    "                \"centralization_top3\", \"avg_degree\", \"avg_closeness\", \"avg_pagerank\"\n",
    "            ]}\n",
    "\n",
    "        results = {\"n_nodes\": n_nodes, \"n_edges\": n_edges}\n",
    "\n",
    "        # 1. Density\n",
    "        results[\"density\"] = nx.density(H)\n",
    "\n",
    "        # 2. Clustering\n",
    "        results[\"clustering\"] = nx.average_clustering(H, weight=\"weight\")\n",
    "\n",
    "        # 3. Modularity\n",
    "        modularity = np.nan\n",
    "        try:\n",
    "            if n_edges > 0 and n_nodes > 2:\n",
    "                import community as community_louvain\n",
    "                partition = community_louvain.best_partition(H, weight=\"weight\", random_state=seed)\n",
    "                modularity = community_louvain.modularity(partition, H, weight=\"weight\")\n",
    "        except:\n",
    "            pass\n",
    "        results[\"modularity\"] = modularity\n",
    "\n",
    "        # 4. Centralization (top 3)\n",
    "        deg = dict(H.degree(weight=\"weight\"))\n",
    "        degvals = sorted(deg.values(), reverse=True)\n",
    "        results[\"centralization_top3\"] = sum(degvals[:3]) / (sum(degvals) + 1e-12)\n",
    "\n",
    "        # 5. Avg Degree\n",
    "        degree_c = nx.degree_centrality(H)\n",
    "        results[\"avg_degree\"] = np.mean(list(degree_c.values()))\n",
    "\n",
    "        # 6. Avg Closeness (sampled)\n",
    "        if n_nodes > 10000000:\n",
    "            nodes_sample = random.sample(list(H.nodes()), int(n_nodes * sample_frac))\n",
    "            closeness_c = nx.closeness_centrality(H.subgraph(nodes_sample))\n",
    "        else:\n",
    "            closeness_c = nx.closeness_centrality(H)\n",
    "        results[\"avg_closeness\"] = np.mean(list(closeness_c.values()))\n",
    "\n",
    "        # 7. Avg PageRank\n",
    "        pagerank_c = nx.pagerank(H, weight=\"weight\", max_iter=100)\n",
    "        results[\"avg_pagerank\"] = np.mean(list(pagerank_c.values()))\n",
    "\n",
    "        return results\n",
    "\n",
    "    # ==============================================================\n",
    "    # COMPUTE & PRINT METRICS (ROBUST TO MISSING COVID)\n",
    "    # ==============================================================\n",
    "    metrics_full = graph_metrics(G)\n",
    "    \n",
    "    # Safely handle missing COVID ego\n",
    "    if covid_ego is not None and covid_ego.number_of_nodes() > 0:\n",
    "        metrics_covid = graph_metrics(covid_ego)\n",
    "        covid_exists = True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No COVID ego network ‚Äî using NaN for COVID metrics.\")\n",
    "        metrics_covid = {k: np.nan for k in metrics_full.keys()}\n",
    "        covid_exists = False\n",
    "\n",
    "    # Safe comparison table\n",
    "    comparison = pd.DataFrame({\n",
    "        \"Metric\": metrics_full.keys(),\n",
    "        \"Full\": metrics_full.values(),\n",
    "        \"COVID\": list(metrics_covid.values()),\n",
    "    })\n",
    "    comparison[\"COVID/Full\"] = comparison[\"COVID\"] / (comparison[\"Full\"] + 1e-12)\n",
    "    print(comparison.round(4))\n",
    "\n",
    "\n",
    "    # === SETTINGS (adjust if needed) ===\n",
    "    N_PERMUTATIONS = 1000\n",
    "    RANDOM_STATE = 42\n",
    "    METRICS_TO_TEST = [\n",
    "        \"n_nodes\",\n",
    "        \"n_edges\",\n",
    "        \"density\",\n",
    "        \"clustering\",\n",
    "        \"modularity\",\n",
    "        \"centralization_top3\",\n",
    "        \"avg_degree\",\n",
    "        \"avg_closeness\",\n",
    "        \"avg_pagerank\"\n",
    "    ]\n",
    "\n",
    "    # ==============================================================\n",
    "    # PERMUTATION TEST (ONLY IF COVID EGO EXISTS)\n",
    "    # ==============================================================\n",
    "    if covid_exists:\n",
    "        print(f\"\\nRunning {N_PERMUTATIONS} permutations...\")\n",
    "        observed_size = len(covid_ego.nodes())\n",
    "        obs_metrics = metrics_covid\n",
    "        perm_results = {m: {\"null\": [], \"obs\": obs_metrics.get(m, np.nan)} for m in METRICS_TO_TEST}\n",
    "        rng = np.random.RandomState(RANDOM_STATE)\n",
    "        all_nodes = list(G.nodes())\n",
    "\n",
    "        summary_rows = []\n",
    "        for i in tqdm(range(N_PERMUTATIONS), desc=\"Permutations\", leave=False, ncols=80):\n",
    "            sample_nodes = set(rng.choice(all_nodes, size=observed_size, replace=False))\n",
    "            sG = G.subgraph(sample_nodes).copy()\n",
    "            m = graph_metrics(sG)\n",
    "            for metric in METRICS_TO_TEST:\n",
    "                perm_results[metric][\"null\"].append(m.get(metric, np.nan))\n",
    "\n",
    "        for metric in METRICS_TO_TEST:\n",
    "            null_vals = np.array(perm_results[metric][\"null\"], dtype=np.float64)\n",
    "            null_vals = null_vals[~np.isnan(null_vals)]\n",
    "            obs_val = perm_results[metric][\"obs\"]\n",
    "            if len(null_vals) == 0 or np.isnan(obs_val):\n",
    "                p_val = np.nan; null_mean = np.nan; null_std = np.nan\n",
    "            else:\n",
    "                null_mean = float(null_vals.mean())\n",
    "                null_std = float(null_vals.std(ddof=1))\n",
    "                p_val = (np.sum(np.abs(null_vals - null_mean) >= abs(obs_val - null_mean)) + 1) / (len(null_vals) + 1)\n",
    "            summary_rows.append({\n",
    "                \"metric\": metric,\n",
    "                \"obs_value\": obs_val,\n",
    "                \"null_mean\": null_mean,\n",
    "                \"null_std\": null_std,\n",
    "                \"p_two_sided\": p_val,\n",
    "                \"obs_minus_null_mean\": (obs_val - null_mean) if not np.isnan(obs_val) else np.nan,\n",
    "                \"ratio_to_null_mean\": (obs_val / (null_mean + 1e-12)) if not np.isnan(obs_val) else np.nan,\n",
    "                \"n_perm\": len(null_vals)\n",
    "            })\n",
    "        summary_df = pd.DataFrame(summary_rows)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping permutation test ‚Äî no COVID ego network.\")\n",
    "        summary_df = pd.DataFrame([{\n",
    "            \"metric\": m,\n",
    "            \"obs_value\": np.nan,\n",
    "            \"null_mean\": np.nan,\n",
    "            \"null_std\": np.nan,\n",
    "            \"p_two_sided\": np.nan,\n",
    "            \"obs_minus_null_mean\": np.nan,\n",
    "            \"ratio_to_null_mean\": np.nan,\n",
    "            \"n_perm\": 0\n",
    "        } for m in METRICS_TO_TEST])\n",
    "\n",
    "    # ==============================================================\n",
    "    # SAVE NETWORKS + RESULTS (ALWAYS RUN)\n",
    "    # ==============================================================\n",
    "    print(f\"\\nSaving outputs to: {ELITE_OUT_DIR}\")\n",
    "\n",
    "    # 1. Save Full Graph (ALWAYS)\n",
    "    try:\n",
    "        full_gml_path = ELITE_OUT_DIR / f\"{elite_name}_FullGraph.gml\"\n",
    "        nx.write_gml(G, full_gml_path)\n",
    "        print(f\"‚úÖ FullGraph.gml saved ({full_gml_path.stat().st_size:,} bytes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save FullGraph.gml: {e}\")\n",
    "\n",
    "    # 2. Save COVID Ego (ONLY IF EXISTS)\n",
    "    if covid_exists:\n",
    "        try:\n",
    "            covid_gml_path = ELITE_OUT_DIR / f\"{elite_name}_COVIDego.gml\"\n",
    "            nx.write_gml(covid_ego, covid_gml_path)\n",
    "            print(f\"‚úÖ COVIDego.gml saved\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to save COVIDego.gml: {e}\")\n",
    "\n",
    "        # Save COVID PNG\n",
    "        try:\n",
    "            visualize_graph(covid_ego, f\"COVID Ego Network ‚Äî {elite_name}\", f\"{elite_name}_COVIDego.png\", out_dir=ELITE_OUT_DIR)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to save COVIDego.png: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No COVID ego ‚Üí skipping COVIDego.gml and .png\")\n",
    "\n",
    "    # 3. Save Excel Report (ALWAYS)\n",
    "    try:\n",
    "        comparison_df = comparison.copy()\n",
    "        comparison_df.rename(columns={\"Metric\": \"metric\"}, inplace=True)\n",
    "        comparison_df.set_index(\"metric\", inplace=True)\n",
    "\n",
    "        combined = comparison_df.join(summary_df.set_index(\"metric\"), how=\"outer\")\n",
    "        excel_path = ELITE_OUT_DIR / f\"{elite_name}_Centrality_and_PermutationSummary.xlsx\"\n",
    "\n",
    "        with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "            combined.to_excel(writer, sheet_name=\"Comparison + Permutation\")\n",
    "            comparison_df.to_excel(writer, sheet_name=\"COVID_vs_Full\")\n",
    "            summary_df.to_excel(writer, sheet_name=\"Permutation_Summary\")\n",
    "\n",
    "        print(f\"‚úÖ Excel report saved: {excel_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save Excel report: {e}\")\n",
    "        import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# RUN PIPELINE FOR ALL ELITES (start with Politicians)\n",
    "# ==============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "subfolders_in_order = [\"Politicians\", \"Businessmen\", \"Celebrities\"]\n",
    "\n",
    "excel_files = []\n",
    "for sub in subfolders_in_order:\n",
    "    sub_dir = BASE_INPUT_DIR / sub\n",
    "    # ‚úÖ this pattern finds files inside each Elite's \"Output\" subfolder\n",
    "    excel_files.extend(sorted(sub_dir.rglob(\"Output/*.xlsx\")))\n",
    "\n",
    "print(f\"üìä Found {len(excel_files)} elite Excel files to process (starting with Politicians).\")\n",
    "\n",
    "\n",
    "for i, file_path in enumerate(excel_files, start=1):\n",
    "    elite_name = file_path.stem\n",
    "    elite_output_dir = BASE_OUTPUT_DIR / elite_name\n",
    "    elite_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if any(elite_output_dir.iterdir()):\n",
    "        print(f\"Skipping {elite_name} (already processed)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n[{i}/{len(excel_files)}] Processing: {elite_name}\")\n",
    "    try:\n",
    "        analyze_elite(file_path, elite_output_dir, INTERVENTION_DATE)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {elite_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "\n",
    "print(\"\\nüéØ All elites processed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
