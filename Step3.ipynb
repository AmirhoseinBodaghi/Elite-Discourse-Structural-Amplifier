{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274bfecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5bed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "base_dir = Path(r\" \")\n",
    "input_root = base_dir / \"Step2\"\n",
    "output_root = base_dir / \"Step3\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# intervention week (use week containing 2020-03-11)\n",
    "intervention_date = pd.to_datetime(\"2020-03-11\")\n",
    "global_start = pd.to_datetime(\"2010-01-04\")\n",
    "global_end   = pd.to_datetime(\"2021-12-26\")\n",
    "weekly_index = pd.date_range(start=global_start, end=global_end, freq=\"W-MON\")\n",
    "\n",
    "def week_start(ts):\n",
    "    return ts.to_period(\"W-SUN\").start_time if pd.notna(ts) else pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbad444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# CATEGORY MAPPING\n",
    "# =========================================================\n",
    "category_map = {\n",
    "    # Politicians & world leaders\n",
    "    \"Politicians and World Leaders\": [\n",
    "        \"Abdelfattah Elsisi\",\"Alberto Fern√°ndez\",\"Barack Obama\",\"Benjamin Netanyahu\",\"Boris Johnson\",\n",
    "        \"Emmanuel Macron\",\"Imran Khan\",\"Ivan Duque\",\"Jair Bolsonaro\",\"Joko Widodo\",\"Justin Trudeau\",\n",
    "        \"Kaguta Museveni\",\"King Salman\",\"Lopez Obrador\",\"Mohammed AlMaktoum\",\"Moon Jaein\",\n",
    "        \"Muhammadu Buhari\",\"Nana AkufoAddo\",\"Narendra Modi\",\"Nicolas Maduro\",\"Paul Kagame\",\"Queen Rania\",\"Sebastian Pinera\",\"Tayyip Erdogan\"\n",
    "    ],\n",
    "\n",
    "    # Business / entrepreneurs\n",
    "    \"Business Leaders and Entrepreneurs\": [\n",
    "        \"Bill Gates\",\"Elon Musk\",\"Eric Yuan\",\"Jack Dorsey\",\"Joe Gebbia\",\"John Collison\",\"Ken Fisher\",\n",
    "        \"Marc Benioff\",\"Melinda Gates\",\"Michael Dell\",\"Micky Arison\",\"Mike Bloomberg\",\"Mike Brookes\",\n",
    "        \"Orlando Bravo\",\"Patrick Collison\",\"Ralph Lauren\",\"Ricardo Salinas\",\"Samuel Bankman\",\n",
    "        \"Tilman Fertitta\",\"Tim Sweeney\",\"Tobi Lutke\",\"Vinod Khosla\"\n",
    "    ],\n",
    "\n",
    "    # Entertainment umbrella: actors, athletes, comedians, models\n",
    "    \"Entertainers and Celebrities\": [\n",
    "        # singers, actors, athletes, comedians, reality stars\n",
    "        \"Adele Adkins\",\"Alecia Beth\",\"Alicia Keys\",\"Armando Perez\",\"Britney Spears\",\"Bruno Mars\",\n",
    "        \"Chris Brown\",\"Demi Lovato\",\"Drake Graham\",\"Harry Styles\",\"Jennifer Lopez\",\"Justin Bieber\",\n",
    "        \"Justin Timberlake\",\"Kanye West\",\"Katy Perry\",\"Lady Gaga\",\"Liam Payne\",\"Lil Wayne\",\n",
    "        \"Louis Tomlinson\",\"Miley Cyrus\",\"Niall Horan\",\"Nicki Minaj\",\"Robyn Rihanna\",\"Shakira Ripoll\",\n",
    "        \"Shawn Mendes\",\"Taylor Swift\",\"Wiz Khalifa\",\"Zayn Malik\",\n",
    "        \"Akshay Kumar\",\"Amitabh Bachchan\",\"Deepika Padukone\",\"Emma Watson\",\"Hrithik Roshan\",\n",
    "        \"Priyanka Chopra\",\"Salman Khan\",\"Selena Gomez\",\"ShahRukh Khan\",\n",
    "        \"Andres Iniesta\",\"Cristiano Ronaldo\",\"LeBron James\",\"Mesut Ozil\",\"Neymar Junior\",\n",
    "        \"Ricardo Kaka\",\"Sachin Tendulkar\",\"Virat Kohli\",\n",
    "        \"Conan Obrien\",\"Jimmy Fallon\",\"Kevin Hart\",\"Oprah Winfrey\",\"Patrick Harris\",\"Whindersson Nunes\",\n",
    "        \"Kendal Jenner\",\"Khloe Kardashian\",\"Kim Kardashian\",\"Kourtney Kardashian\",\"Kylie Jenner\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# reverse map for lookup\n",
    "person_to_cat = {p: c for c, plist in category_map.items() for p in plist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cdd6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# BUILD BALANCED PANEL (FIXED VERSION)\n",
    "# =========================================================\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    \"\"\"Normalize a name by removing spaces, underscores, and making lowercase.\"\"\"\n",
    "    return name.replace(\" \", \"\").replace(\"_\", \"\").lower()\n",
    "\n",
    "# Build normalized lookup dictionary from category_map\n",
    "person_to_cat = {\n",
    "    normalize_name(person): category\n",
    "    for category, people in category_map.items()\n",
    "    for person in people\n",
    "}\n",
    "\n",
    "panel_rows = []\n",
    "\n",
    "for folder in input_root.glob(\"*\"):\n",
    "    for f in folder.glob(\"*.xlsx\"):\n",
    "        # Load file\n",
    "        df = pd.read_excel(f)\n",
    "        df[\"week\"] = pd.to_datetime(df[\"week\"], errors=\"coerce\").dt.normalize()\n",
    "        \n",
    "        # Ensure every person has all weeks (balanced panel)\n",
    "        reindexed = pd.DataFrame({\"week\": weekly_index}).merge(df, on=\"week\", how=\"left\")\n",
    "        reindexed[\"tweet_count\"] = reindexed[\"tweet_count\"].fillna(0).astype(int)\n",
    "        \n",
    "        # Extract and normalize person name\n",
    "        person = f.stem.replace(\"_Weekly_Behavioral_Summary\", \"\")\n",
    "        person_norm = normalize_name(person)\n",
    "        \n",
    "        # Assign category using normalized lookup\n",
    "        category = person_to_cat.get(person_norm, \"Unknown\")\n",
    "        \n",
    "        # Add columns\n",
    "        reindexed[\"person\"] = person\n",
    "        reindexed[\"category\"] = category\n",
    "        \n",
    "        panel_rows.append(reindexed)\n",
    "\n",
    "# Combine all people into one panel DataFrame\n",
    "panel = pd.concat(panel_rows, ignore_index=True)\n",
    "\n",
    "# Add time-related variables\n",
    "panel[\"time\"] = ((panel[\"week\"] - global_start) / pd.Timedelta(weeks=1)).astype(int)\n",
    "panel[\"intervention\"] = (panel[\"week\"] >= week_start(intervention_date)).astype(int)\n",
    "panel[\"time_after\"] = panel[\"time\"] * panel[\"intervention\"]\n",
    "panel[\"person_str\"] = panel[\"person\"].astype(str)\n",
    "\n",
    "# Save panel\n",
    "output_path = output_root / \"All_People_Weekly_Balanced_Panel.csv\"\n",
    "panel.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Panel ready: {panel.shape}\")\n",
    "print(panel[\"category\"].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# RUN ITS MODELS (BY CATEGORY)\n",
    "# =========================================================\n",
    "def fit_model(df, formula, family=None):\n",
    "    if family:\n",
    "        model = smf.glm(formula, data=df, family=family).fit(\n",
    "            cov_type=\"cluster\", cov_kwds={\"groups\": df[\"person\"]}\n",
    "        )\n",
    "    else:\n",
    "        model = smf.ols(formula, data=df).fit(\n",
    "            cov_type=\"cluster\", cov_kwds={\"groups\": df[\"person\"]}\n",
    "        )\n",
    "    return model\n",
    "\n",
    "outcomes = [\"tweet_count\",\"avg_sentiment\",\"avg_length\",\n",
    "            \"prop_tweet\",\"prop_retweet\",\"prop_reply\",\"prop_quote\"]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for cat, df_cat in panel.groupby(\"category\"):\n",
    "    print(f\"\\nüìä Category: {cat} (n={df_cat.person.nunique()} people)\")\n",
    "    for var in outcomes:\n",
    "        if var not in df_cat.columns: continue\n",
    "        df_sub = df_cat.dropna(subset=[var])\n",
    "        family = sm.families.Poisson() if var==\"tweet_count\" else None\n",
    "        model = fit_model(df_sub, f\"{var} ~ time + intervention + time_after + C(person_str)\", family)\n",
    "        resfile = output_root / f\"ITS_{var}_{cat.replace(' ','_')}_summary.txt\"\n",
    "        with open(resfile,\"w\",encoding=\"utf-8\") as f: f.write(model.summary().as_text())\n",
    "        all_results.append((cat,var,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# EXTRACT COEFFICIENTS & SIGNIFICANCE\n",
    "# =========================================================\n",
    "summary_data = []\n",
    "for cat,var,model in all_results:\n",
    "    for term in [\"intervention\",\"time\",\"time_after\"]:\n",
    "        if term in model.params:\n",
    "            summary_data.append({\n",
    "                \"category\":cat,\n",
    "                \"variable\":var,\n",
    "                \"term\":term,\n",
    "                \"coef\":model.params[term],\n",
    "                \"std_err\":model.bse[term],\n",
    "                \"p_value\":model.pvalues[term],\n",
    "                \"z\":model.tvalues[term]\n",
    "            })\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# significance stars\n",
    "def stars(p):\n",
    "    return \"***\" if p<0.001 else \"**\" if p<0.01 else \"*\" if p<0.05 else \"\"\n",
    "\n",
    "summary_df[\"sig\"] = summary_df[\"p_value\"].apply(stars)\n",
    "summary_df[\"coef_sig\"] = summary_df[\"coef\"].round(4).astype(str)+summary_df[\"sig\"]\n",
    "\n",
    "summary_path = output_root / \"ITS_summary_by_category.csv\"\n",
    "summary_df.to_csv(summary_path,index=False)\n",
    "print(f\"‚úÖ Saved category-level summary to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# PERSON-LEVEL META REGRESSION (robust)\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# outcomes to meta-analyze (same names used earlier)\n",
    "outcomes = [\"tweet_count\",\"avg_sentiment\",\"avg_length\",\n",
    "            \"prop_tweet\",\"prop_retweet\",\"prop_reply\",\"prop_quote\"]\n",
    "\n",
    "# Ensure person->category mapping normalized (if you used normalized mapping earlier)\n",
    "# person_to_cat: keys are exact person strings as stored in panel[\"person\"]\n",
    "# followers: dict mapping display name -> follower count (you already have this)\n",
    "\n",
    "# Build raw meta rows: for each person & outcome, fit simple ITS and keep coefficients\n",
    "meta_rows = []\n",
    "min_rows_required = 15  # you used 15 earlier\n",
    "\n",
    "for var in outcomes:\n",
    "    print(f\"\\nüîç Building person-level ITS stats for: {var}\")\n",
    "    for person, d in panel.groupby(\"person\"):\n",
    "        # require enough non-NaN data points for this person for this variable\n",
    "        if d[var].notna().sum() < min_rows_required:\n",
    "            continue\n",
    "        # dropna on covariates (time/intervention) too\n",
    "        dsub = d.dropna(subset=[var, \"time\", \"intervention\", \"time_after\"])\n",
    "        if dsub.shape[0] < min_rows_required:\n",
    "            continue\n",
    "        try:\n",
    "            m = smf.ols(f\"{var} ~ time + intervention + time_after\", data=dsub).fit()\n",
    "            meta_rows.append({\n",
    "                \"person\": person,\n",
    "                \"variable\": var,\n",
    "                \"n_obs\": dsub.shape[0],\n",
    "                \"intervention_coef\": m.params.get(\"intervention\", np.nan),\n",
    "                \"time_coef\": m.params.get(\"time\", np.nan),\n",
    "                \"time_after_coef\": m.params.get(\"time_after\", np.nan),\n",
    "                \"intervention_se\": m.bse.get(\"intervention\", np.nan),\n",
    "                \"time_se\": m.bse.get(\"time\", np.nan),\n",
    "                \"time_after_se\": m.bse.get(\"time_after\", np.nan),\n",
    "                \"intervention_p\": m.pvalues.get(\"intervention\", np.nan),\n",
    "                \"time_p\": m.pvalues.get(\"time\", np.nan),\n",
    "                \"time_after_p\": m.pvalues.get(\"time_after\", np.nan),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # log optional: print(person, var, \"err\", e)\n",
    "            pass\n",
    "\n",
    "meta_df = pd.DataFrame(meta_rows)\n",
    "print(\"Meta rows shape:\", meta_df.shape)\n",
    "if meta_df.empty:\n",
    "    raise SystemExit(\"No person-level ITS results were produced. Check `panel` and min_rows_required.\")\n",
    "\n",
    "\n",
    "\n",
    "# attach person traits\n",
    "followers = {\n",
    "    \"Abdelfattah Elsisi\": 6300000,\n",
    "    \"Alberto Fern√°ndez\": 2200000,\n",
    "    \"Barack Obama\": 129000000,\n",
    "    \"Benjamin Netanyahu\": 3500000,\n",
    "    \"Boris Johnson\": 4500000,\n",
    "    \"Emmanuel Macron\": 10200000,\n",
    "    \"Imran Khan\": 21200000,\n",
    "    \"Ivan Duque\": 2600000,\n",
    "    \"Jair Bolsonaro\": 14100000,\n",
    "    \"Joko Widodo\": 21700000,\n",
    "    \"Justin Trudeau\": 6600000,\n",
    "    \"Kaguta Museveni\": 3600000,\n",
    "    \"King Salman\": 10200000,\n",
    "    \"Lopez Obrador\": 11000000,\n",
    "    \"Mohammed AlMaktoum\": 10900000,\n",
    "    \"Moon Jaein\": 2100000,\n",
    "    \"Muhammadu Buhari\": 4500000,\n",
    "    \"Nana AkufoAddo\": 2700000,\n",
    "    \"Narendra Modi\": 108600000,\n",
    "    \"Nicolas Maduro\": 4700000,\n",
    "    \"Paul Kagame\": 3300000,\n",
    "    \"Queen Rania\": 9700000,\n",
    "    \"Sebastian Pinera\": 2300000,\n",
    "    \"Tayyip Erdogan\": 21300000,\n",
    "    \"Adele Adkins\": 25500000,\n",
    "    \"Alecia Beth\": 28700000,\n",
    "    \"Alicia Keys\": 27700000,\n",
    "    \"Armando Perez\": 22800000,\n",
    "    \"Britney Spears\": 51600000,\n",
    "    \"Bruno Mars\": 40900000,\n",
    "    \"Chris Brown\": 30500000,\n",
    "    \"Demi Lovato\": 50000000,\n",
    "    \"Drake Graham\": 37800000,\n",
    "    \"Harry Styles\": 35300000,\n",
    "    \"Jennifer Lopez\": 42200000,\n",
    "    \"Justin Bieber\": 105700000,\n",
    "    \"Justin Timberlake\": 57400000,\n",
    "    \"Kanye West\": 32700000,\n",
    "    \"Katy Perry\": 101300000,\n",
    "    \"Lady Gaga\": 79700000,\n",
    "    \"Liam Payne\": 31600000,\n",
    "    \"Lil Wayne\": 32900000,\n",
    "    \"Louis Tomlinson\": 33500000,\n",
    "    \"Miley Cyrus\": 44600000,\n",
    "    \"Niall Horan\": 38200000,\n",
    "    \"Nicki Minaj\": 27800000,\n",
    "    \"Robyn Rihanna\": 105800000,\n",
    "    \"Shakira Ripoll\": 51300000,\n",
    "    \"Shawn Mendes\": 25200000,\n",
    "    \"Taylor Swift\": 91400000,\n",
    "    \"Wiz Khalifa\": 34700000,\n",
    "    \"Zayn Malik\": 29200000,\n",
    "    \"Akshay Kumar\": 46600000,\n",
    "    \"Amitabh Bachchan\": 48300000,\n",
    "    \"Deepika Padukone\": 25700000,\n",
    "    \"Emma Watson\": 25700000,\n",
    "    \"Hrithik Roshan\": 31800000,\n",
    "    \"Priyanka Chopra\": 26800000,\n",
    "    \"Salman Khan\": 45200000,\n",
    "    \"Selena Gomez\": 63600000,\n",
    "    \"ShahRukh Khan\": 43500000,\n",
    "    \"Andres Iniesta\": 24300000,\n",
    "    \"Cristiano Ronaldo\": 114200000,\n",
    "    \"LeBron James\": 52000000,\n",
    "    \"Mesut Ozil\": 25300000,\n",
    "    \"Neymar Junior\": 62900000,\n",
    "    \"Ricardo Kaka\": 27400000,\n",
    "    \"Sachin Tendulkar\": 40500000,\n",
    "    \"Virat Kohli\": 67500000,\n",
    "    \"Conan Obrien\": 26100000,\n",
    "    \"Jimmy Fallon\": 47800000,\n",
    "    \"Kevin Hart\": 35200000,\n",
    "    \"Oprah Winfrey\": 39400000,\n",
    "    \"Patrick Harris\": 23100000,\n",
    "    \"Whindersson Nunes\": 27600000,\n",
    "    \"Kendal Jenner\": 30600000,\n",
    "    \"Khloe Kardashian\": 29200000,\n",
    "    \"Kim Kardashian\": 73300000,\n",
    "    \"Kourtney Kardashian\": 25200000,\n",
    "    \"Kylie Jenner\": 38900000,\n",
    "    \"Bill Gates\": 66000000,\n",
    "    \"Elon Musk\": 228500000,\n",
    "    \"Eric Yuan\": 86800,\n",
    "    \"Jack Dorsey\": 6400000,\n",
    "    \"Joe Gebbia\": 206100,\n",
    "    \"John Collison\": 208500,\n",
    "    \"Ken Fisher\": 428300,\n",
    "    \"Marc Benioff\": 1100000,\n",
    "    \"Melinda Gates\": 2400000,\n",
    "    \"Michael Dell\": 796100,\n",
    "    \"Micky Arison\": 184800,\n",
    "    \"Mike Bloomberg\": 2500000,\n",
    "    \"Mike Brookes\": 105100,\n",
    "    \"Orlando Bravo\": 44100,\n",
    "    \"Patrick Collison\": 572000,\n",
    "    \"Ralph Lauren\": 2200000,\n",
    "    \"Ricardo Salinas\": 2100000,\n",
    "    \"Samuel Bankman\": 996800,\n",
    "    \"Tilman Fertitta\": 103800,\n",
    "    \"Tim Sweeney\": 316100,\n",
    "    \"Tobi Lutke\": 424100,\n",
    "    \"Vinod Khosla\": 686500\n",
    "\n",
    "    # ... include all others or read from a CSV later\n",
    "}\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "def normalize_name(name):\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    # remove accents and normalize\n",
    "    name = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', name)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "    # remove spaces and lowercase\n",
    "    return name.replace(\" \", \"\").lower()\n",
    "\n",
    "# normalize keys in the followers dictionary\n",
    "followers_norm = {normalize_name(k): v for k, v in followers.items()}\n",
    "\n",
    "# normalize names in the meta_df\n",
    "meta_df[\"person_norm\"] = meta_df[\"person\"].apply(normalize_name)\n",
    "\n",
    "# map using normalized names\n",
    "meta_df[\"followers\"] = meta_df[\"person_norm\"].map(followers_norm)\n",
    "meta_df[\"log_followers\"] = np.log1p(meta_df[\"followers\"])\n",
    "\n",
    "# Map category: if person_to_cat keys are normalized, adapt similarly\n",
    "# If person_to_cat keys are normalized names, normalize the meta_df.person similarly\n",
    "# We'll try both direct map and normalized fallback:\n",
    "def normalize_person_for_lookup(name):\n",
    "    return name.replace(\" \", \"\").replace(\"_\", \"\").lower()\n",
    "\n",
    "# try direct mapping first\n",
    "meta_df[\"category\"] = meta_df[\"person\"].map(person_to_cat)\n",
    "\n",
    "# if many Unknown, try normalized mapping\n",
    "if meta_df[\"category\"].isna().sum() > 0:\n",
    "    # build normalized person_to_cat_norm (if not already built)\n",
    "    person_to_cat_norm = {}\n",
    "    for k, v in person_to_cat.items():\n",
    "        # if keys are display names, they may already be normalized; handle both\n",
    "        person_to_cat_norm[normalize_person_for_lookup(k)] = v\n",
    "        person_to_cat_norm[normalize_person_for_lookup(k.replace(\"_Weekly_Behavioral_Summary\",\"\"))] = v\n",
    "    # map using normalized key\n",
    "    meta_df.loc[meta_df[\"category\"].isna(), \"category\"] = meta_df.loc[meta_df[\"category\"].isna(), \"person\"].apply(\n",
    "        lambda x: person_to_cat_norm.get(normalize_person_for_lookup(x), \"Unknown\")\n",
    "    )\n",
    "\n",
    "# Compute pre-COVID averages per person for the same variables\n",
    "pre = panel[panel[\"week\"] < week_start(intervention_date)]\n",
    "pre_means = pre.groupby(\"person\")[[ \"tweet_count\",\"avg_sentiment\",\"avg_length\",\n",
    "                                    \"prop_tweet\",\"prop_retweet\",\"prop_reply\",\"prop_quote\"]].mean()\n",
    "pre_means = pre_means.add_prefix(\"pre_\")\n",
    "\n",
    "# Merge pre-period means into meta_df\n",
    "meta_df = meta_df.merge(pre_means, left_on=\"person\", right_index=True, how=\"left\")\n",
    "\n",
    "# --- Run meta-regression per outcome variable ---\n",
    "meta_results = []\n",
    "for var in outcomes:\n",
    "    df_var = meta_df[meta_df[\"variable\"] == var].copy()\n",
    "    if df_var.shape[0] < 10:\n",
    "        print(f\"‚ö†Ô∏è Skipping meta-regression for {var}: only {df_var.shape[0]} persons\")\n",
    "        continue\n",
    "\n",
    "    # prepare formula: outcome is intervention_coef (level shift) or time_after_coef (slope change)\n",
    "    # We'll do two meta-regressions per outcome: (A) intervention_coef as dependent, (B) time_after_coef as dependent\n",
    "    for dep in [\"intervention_coef\", \"time_after_coef\"]:\n",
    "        # Basic predictiors to include (only keep columns that exist and are not all-NaN)\n",
    "        predictors = [\"log_followers\", \"pre_avg_sentiment\", \"pre_avg_length\", \"pre_tweet_count\"]\n",
    "        # rename pre columns mapping expected names\n",
    "        rename_map = {\n",
    "            \"pre_avg_sentiment\": \"pre_avg_sentiment\",  # but our pre_means column is pre_avg_sentiment? match names below\n",
    "        }\n",
    "        # The pre_means columns are named like pre_avg_sentiment? we created pre_ + variable names: pre_avg_sentiment, pre_tweet_count etc\n",
    "        # ensure they exist:\n",
    "        available_preds = []\n",
    "        for p in [\"log_followers\", \"pre_avg_sentiment\", \"pre_avg_length\", \"pre_tweet_count\"]:\n",
    "            col = p\n",
    "            # pre_ columns in meta_df are: pre_avg_sentiment is actually pre_avg_sentiment? check:\n",
    "            # We created pre_ + original column names -> pre_avg_sentiment exists because original column name was avg_sentiment\n",
    "            if col in df_var.columns or col in meta_df.columns:\n",
    "                available_preds.append(col)\n",
    "\n",
    "        # Clean df_var: drop rows missing dep or predictors\n",
    "        cols_needed = [dep] + available_preds + [\"category\"]\n",
    "        df_clean = df_var[cols_needed].dropna()\n",
    "        nobs = df_clean.shape[0]\n",
    "        if nobs < 8:\n",
    "            print(f\" ‚ö†Ô∏è Not enough complete rows for {var} / {dep} ({nobs} rows). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # If category has only one level in df_clean, drop it from formula\n",
    "        cat_levels = df_clean[\"category\"].dropna().unique().tolist()\n",
    "        include_cat = (len(cat_levels) > 1 and not (len(cat_levels) == 1 and cat_levels[0] in [\"\", \"Unknown\"]))\n",
    "        if include_cat:\n",
    "            formula = dep + \" ~ \" + \" + \".join(available_preds) + \" + C(category)\"\n",
    "        else:\n",
    "            formula = dep + \" ~ \" + \" + \".join(available_preds)\n",
    "\n",
    "        try:\n",
    "            mf = smf.ols(formula, data=df_clean).fit()\n",
    "            meta_results.append({\n",
    "                \"outcome_var\": var,\n",
    "                \"dependent\": dep,\n",
    "                \"n_persons\": nobs,\n",
    "                \"formula\": formula,\n",
    "                \"params\": mf.params.to_dict(),\n",
    "                \"bse\": mf.bse.to_dict(),\n",
    "                \"pvalues\": mf.pvalues.to_dict(),\n",
    "                \"rsquared\": mf.rsquared\n",
    "            })\n",
    "            print(f\" ‚úÖ Meta {var} | {dep}  ‚Äî n={nobs} formula: {formula}\")\n",
    "        except Exception as e:\n",
    "            print(f\" ‚ùå Meta regression failed for {var} {dep}: {e}\")\n",
    "\n",
    "# Save meta-level data and results\n",
    "meta_df.to_csv(output_root / \"ITS_meta_person_level_rows.csv\", index=False)\n",
    "import json\n",
    "with open(output_root / \"ITS_meta_regression_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta_results, f, indent=2, default=str)\n",
    "\n",
    "print(\"‚úÖ Saved meta person-level rows and meta-regression results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddbb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# ‚ú® FINAL POLISH: Thicker Confidence Lines + Significance Heatmap\n",
    "# =========================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "light_palette = [\"#f4a582\", \"#92c5de\", \"#b2df8a\"]\n",
    "\n",
    "def plot_symlog_bar_final(term, df):\n",
    "    df_plot = df[df[\"term\"] == term].copy()\n",
    "\n",
    "    vars_order = list(df_plot[\"variable\"].unique())\n",
    "    df_plot[\"variable\"] = pd.Categorical(df_plot[\"variable\"], categories=vars_order, ordered=True)\n",
    "    df_plot = df_plot.sort_values([\"variable\", \"category\"]).reset_index(drop=True)\n",
    "\n",
    "    categories = df_plot[\"category\"].unique()\n",
    "    n_cat = len(categories)\n",
    "    n_vars = len(vars_order)\n",
    "\n",
    "    base_positions = np.arange(n_vars)\n",
    "    offsets = np.linspace(-0.3, 0.3, n_cat)\n",
    "    bar_width = 0.2\n",
    "\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # ---- Draw bars and confidence lines ----\n",
    "    for i, cat in enumerate(categories):\n",
    "        subset = df_plot[df_plot[\"category\"] == cat]\n",
    "        x_positions = base_positions + offsets[i]\n",
    "\n",
    "        ax.bar(\n",
    "            x_positions,\n",
    "            subset[\"coef\"],\n",
    "            width=bar_width,\n",
    "            label=cat,\n",
    "            color=light_palette[i],\n",
    "            alpha=0.9,\n",
    "            edgecolor=\"none\",\n",
    "        )\n",
    "\n",
    "        # Thicker, darker confidence intervals\n",
    "        for x, c, se in zip(x_positions, subset[\"coef\"], subset[\"std_err\"]):\n",
    "            ax.plot(\n",
    "                [x, x],\n",
    "                [c - se, c + se],\n",
    "                color=\"black\",\n",
    "                lw=2,\n",
    "                alpha=0.8,\n",
    "                solid_capstyle=\"round\"\n",
    "            )\n",
    "\n",
    "    # ---- Y-axis (Symmetric log) ----\n",
    "    if term == \"intervention\":\n",
    "        ax.set_yscale(\"symlog\", linthresh=0.01)\n",
    "        yticks = [-1000, -100, -10, -1, -0.1, -0.01,\n",
    "                  0, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    else:\n",
    "        ax.set_yscale(\"symlog\", linthresh=0.0001)\n",
    "        yticks = [-1, -0.1, -0.01, -0.001, -0.0001,\n",
    "                  0, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_yticklabels([str(y) for y in yticks])\n",
    "\n",
    "    # ---- X-axis evenly spaced ----\n",
    "    ax.set_xticks(base_positions)\n",
    "    ax.set_xticklabels(vars_order, rotation=0, ha=\"center\")\n",
    "\n",
    "    # ---- Titles & Labels ----\n",
    "    ax.set_title(f\"{term.title()} Effects (Symmetric Log Y-axis)\", fontsize=18)\n",
    "    ax.set_xlabel(\"Outcome Variable\", fontsize=15)\n",
    "    ax.set_ylabel(\"Effect Size (symlog scale)\", fontsize=15)\n",
    "\n",
    "    ax.legend(title=\"Category\", frameon=True, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_root / f\"Fig_Category_{term}_effects_symlog_FINAL.png\", dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for term in [\"intervention\", \"time_after\"]:\n",
    "    plot_symlog_bar_final(term, summary_df)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# (2) Improved Significance Heatmap (compact labels)\n",
    "# =========================================================\n",
    "# --- Abbreviations for outcome variables ---\n",
    "abbr_map = {\n",
    "    \"tweet_count\": \"Tweets\",\n",
    "    \"avg_sentiment\": \"Sent\",\n",
    "    \"avg_length\": \"Len\",\n",
    "    \"prop_tweet\": \"P_Tw\",\n",
    "    \"prop_retweet\": \"P_RT\",\n",
    "    \"prop_reply\": \"P_Rp\",\n",
    "    \"prop_quote\": \"P_Qt\"\n",
    "}\n",
    "\n",
    "summary_df[\"var_short\"] = summary_df[\"variable\"].map(abbr_map)\n",
    "summary_df[\"cat_term\"] = summary_df[\"category\"] + \"\\n\" + summary_df[\"term\"].str.replace(\"_\", \" \")\n",
    "\n",
    "# --- Create significance bin ---\n",
    "sig_map = (\n",
    "    summary_df\n",
    "    .assign(sig_bin=lambda d: np.select(\n",
    "        [d.p_value < 0.001, d.p_value < 0.01, d.p_value < 0.05],\n",
    "        [3, 2, 1], 0))\n",
    "    .pivot_table(index=\"var_short\", columns=\"cat_term\", values=\"sig_bin\", aggfunc=\"max\")\n",
    ")\n",
    "\n",
    "# --- Plot heatmap ---\n",
    "plt.figure(figsize=(18, 10))  # larger figure for clarity\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    sig_map,\n",
    "    cmap=\"YlGnBu\",\n",
    "    linewidths=0.6,\n",
    "    linecolor=\"white\",\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar_kws={\n",
    "        \"label\": \"Significance Level\\n(3: p < 0.001, 2: p < 0.01, 1: p < 0.05, 0: ns ‚â• 0.05)\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Improve label orientation and clarity ---\n",
    "plt.title(\"Significance Levels by Category √ó Term (Compact, Annotated View)\", fontsize=18, pad=20)\n",
    "plt.xlabel(\"Category √ó Term\", fontsize=15)\n",
    "plt.ylabel(\"Outcome Variable (Abbreviated)\", fontsize=15)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)  # horizontal variable labels\n",
    "\n",
    "# --- Beautify layout ---\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_root / \"Fig_Significance_Heatmap_Improved_Final.png\", dpi=400)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
