{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c52427bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from langdetect import detect, LangDetectException\n",
    "from googletrans import Translator # Assuming googletrans for Translator\n",
    "# from textblob import TextBlob # Assuming you use TextBlob for sentiment if sid is not defined\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer # If you intend to use VADER for sentiment\n",
    "from collections import defaultdict # Keep this if you use the defaultdict for logging duplicates within files\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d711892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download VADER lexicon (run this line only once)\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4960576",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = ' '\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "all_results = [] # This list will collect all processed tweet dictionaries\n",
    "\n",
    "# Recursively traverse through all subfolders and files\n",
    "for root, dirs, files in os.walk(main_folder_path):\n",
    "    # Sort the list of subfolders and files chronologically\n",
    "    dirs.sort()\n",
    "    files.sort()\n",
    "\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.xlsx'):\n",
    "            logging.info(f\"Processing file: {file_name}\") # Changed print to logging.info\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            try:\n",
    "                df = pd.read_excel(file_path, dtype={'id': str})\n",
    "                dfs.append(df)\n",
    "\n",
    "                # Process each tweet in the current DataFrame\n",
    "                for _, row in df.iterrows():\n",
    "                    tweet = row['text']\n",
    "\n",
    "                    if isinstance(tweet, str):\n",
    "                        tweet = tweet.strip()\n",
    "\n",
    "                        if tweet:\n",
    "                            # If you intend to use translation and sentiment, uncomment these lines\n",
    "                            # translated_tweet = process_tweet(tweet)\n",
    "                            # sentiment_scores = sid.polarity_scores(translated_tweet) # Make sure sid is initialized\n",
    "\n",
    "                            all_results.append({\n",
    "                                'creation_datetime': row['created_at'],\n",
    "                                'hashtag': row['hashtag'],\n",
    "                                'mention': row['mention'],\n",
    "                                'Tweet Type': row['Tweet Type'],\n",
    "                                'tweet_id': row['id'], # Use 'id' from the original dataframe\n",
    "                                'text': tweet,\n",
    "                                # 'sentiment_score': sentiment_scores # Uncomment if sentiment is calculated\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error reading or processing file '{file_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your existing duplicate check within individual files (good to keep) ---\n",
    "duplicate_id_files = defaultdict(list)\n",
    "total_dfs = len(dfs) # Define total_dfs here\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    logging.info(f\"Analyzing DataFrame {i+1} of {total_dfs} for internal duplicates.\") # Changed print to logging.info\n",
    "    duplicated_ids = df[df.duplicated('id', keep=False)]\n",
    "    if not duplicated_ids.empty:\n",
    "        file_name = f\"DataFrame_{i+1} (File: {os.path.basename(df.name) if hasattr(df, 'name') else 'Unknown'})\" # Added file name for clarity\n",
    "        for tweet_id in duplicated_ids['id'].unique():\n",
    "            duplicate_id_files[tweet_id].append(file_name)\n",
    "\n",
    "# Print sample problem tweet_ids and the file(s) they appeared in\n",
    "if duplicate_id_files:\n",
    "    logging.warning(f\"Found tweet IDs duplicated WITHIN individual files. Examples:\")\n",
    "    for tweet_id, files in list(duplicate_id_files.items())[:10]:\n",
    "        logging.warning(f\"  Tweet ID {tweet_id} found in files: {files}\")\n",
    "else:\n",
    "    logging.info(\"No tweet IDs duplicated within individual files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the results for all tweets\n",
    "result_df = pd.DataFrame(all_results)\n",
    "\n",
    "# --- REMOVE THE DUPLICATION BLOCK HERE ---\n",
    "# The block for result_df.drop_duplicates(...) should be removed.\n",
    "\n",
    "# Convert tweet_id to string to avoid Excel scientific notation issue\n",
    "# This line is correct and should remain.\n",
    "result_df['tweet_id'] = result_df['tweet_id'].astype(str).apply(lambda x: f\"'{x}'\")\n",
    "\n",
    "# Print or save the results\n",
    "output_excel_path = ' '\n",
    "try:\n",
    "    result_df.to_excel(output_excel_path, index=False, engine='xlsxwriter')\n",
    "    logging.info(f\"Successfully saved combined data to '{output_excel_path}'. Total rows: {len(result_df)}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving output Excel file: {e}\")\n",
    "\n",
    "# --- Your existing duplicate check across the full dataset (for verification) ---\n",
    "# This part is still useful to see if there are duplicates *after* combination,\n",
    "# which is what you observed before, and now you explicitly want to keep them.\n",
    "duplicate_ids_after_combine = result_df[result_df.duplicated('tweet_id', keep=False)]\n",
    "\n",
    "if not duplicate_ids_after_combine.empty:\n",
    "    logging.info(f\"\\n--- Analysis of Duplicated Tweet IDs Across Combined Dataset (as expected) ---\")\n",
    "    logging.info(f\"Total distinct tweet IDs that are duplicated across the combined dataset: {duplicate_ids_after_combine['tweet_id'].nunique()}\")\n",
    "    logging.info(f\"Count of each duplicated ID (showing top 10):\")\n",
    "    logging.info(duplicate_ids_after_combine['tweet_id'].value_counts().head(10))\n",
    "    logging.info(f\"Sample duplicated tweet_ids and texts (showing first 5 entries):\")\n",
    "    logging.info(duplicate_ids_after_combine[['tweet_id', 'text']].head(5))\n",
    "    logging.info(f\"Total rows in final result_df: {len(result_df)}\")\n",
    "else:\n",
    "    logging.info(\"No tweet IDs are duplicated across the combined dataset (this would be unexpected if you had duplicates before).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
